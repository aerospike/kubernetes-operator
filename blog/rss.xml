<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Aerospike Kubernetes Operator Blog</title>
        <link>https://aerospike.github.io/kubernetes-operator/blog</link>
        <description>Aerospike Kubernetes Operator Blog</description>
        <lastBuildDate>Tue, 28 Dec 2021 00:00:00 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <item>
            <title><![CDATA[Aerospike Kubernetes Operator 2.0]]></title>
            <link>https://aerospike.github.io/kubernetes-operator/blog/aerospike-kubernetes-operator2.0</link>
            <guid>aerospike-kubernetes-operator2.0</guid>
            <pubDate>Tue, 28 Dec 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[We are excited to announce the availability of Aerospike Kubernetes Operator 2.0. Kubernetes is becoming an increasingly popular platform for the deployment of Aerospike, particularly within Dev/Ops flows. In the past two years with more staff remote, automation of provisioning, deployment, and management of infrastructure has become essential. Enterprises need to gain leverage in the application of their personnel and technical resources and infrastructure. The Aerospike Kubernetes Operator provides a solid base for automation when deploying and managing our database on Kubernetes platforms, both in the public cloud (AKS, EKS, and GKE) and on-premises.]]></description>
            <content:encoded><![CDATA[<p>We are excited to announce the availability of Aerospike Kubernetes Operator 2.0. Kubernetes is becoming an increasingly popular platform for the deployment of Aerospike, particularly within Dev/Ops flows. In the past two years with more staff remote, automation of provisioning, deployment, and management of infrastructure has become essential. Enterprises need to gain leverage in the application of their personnel and technical resources and infrastructure. The Aerospike Kubernetes Operator provides a solid base for automation when deploying and managing our database on Kubernetes platforms, both in the public cloud (AKS, EKS, and GKE) and on-premises.</p><p>In addition, our Operator provides for improved monitoring and alerting of events in the Aerospike Database. We accomplish this through the Prometheus Exporter Sidecar which can be added to the Aerospike server pods. Combining this with the control plane capabilities of Aerospike Kubernetes Operator 2.0 you have both deployment, observability, and management of Aerospike Database clusters on Kubernetes. There are a number of Grafana dashboards that are configured to work with our Prometheus feeds. Together this provides a sound basis for observability and management of Aerospike clusters on Kubernetes.</p><p>The Operator supports the following capabilities:</p><ul><li>Deploy Aerospike clusters</li><li>Scale up and down for existing Aerospike clusters</li><li>Version upgrade and downgrade</li><li>Configure persistent storage and resource allocation</li><li>Standardize and validate configurations</li><li>Cluster security management</li><li>Attach custom sidecars and init containers</li></ul><h3><strong>New Capabilities</strong></h3><ul><li>Warm restart of pods for cluster changes, allowing for the restart of the Aerospike service without deleting pods.</li><li>Network and Load Balancing<ul><li>Support for LoadBalancer to discover Aerospike externally. The previous headless service allowed for discovery within the cluster. When configured, the Aerospike Kubernetes Operator 2.0 creates a single service for the Aerospike Cluster, with the type LoadBalancer.</li><li>Allow &quot;hostNetwork&quot; in the pod spec section enabling pods to use host networking. This requires the multiPodPerHost configuration to be false.</li><li>An additional &quot;dnsPolicy&quot; configuration added to the podSpec that defaults to ClusterFirstWithHostNet when host network is enabled and is set to ClusterFirst when host network is disabled.</li></ul></li><li>TLS<ul><li>Previously the operator mapped a single secret to all containers in a pod. You can now map secrets to each container within a pod.</li><li>Client certificates can be fed to the operator via files when the Custom Resource Definition AerospikeClientCertSource is set.</li></ul></li><li>Rack Awareness<ul><li>A common practice to support higher availability is to provision clusters to span multiple availability zones. The Aerospike Database supports this with a feature called Rack Awareness. In the new Aerospike Kubernetes Operator, the cluster can be distributed across racks whenever the cluster size is updated or the number of racks is changed. Scheduling policies such as affinity or anti-affinity can be set for each rack.</li></ul></li><li>Custom InitContainer support allows custom initialization of resources like volumes and security certificates.</li><li>Storage Volumes<ul><li>Prior to our 2.0 release, all storage volumes were mounted to all containers. We now provide for more fine-grained control.<ul><li>Allow storage volumes to accept storage volume source, e.g. empty dir, configmap, secret, pv.</li><li>Allow storage volumes to be attached to sidecars and/or init containers.</li></ul></li></ul></li><li>Pod Scheduling<ul><li>Control on Aerospike Pod Scheduling allows specifying affinity, anti-affinity, and tolerations for Aerospike Pods.</li><li>Allow rack-level overrides for scheduling policies.</li></ul></li><li>Install, manage and upgrade the Aerospike Kubernetes Operator 2.0 with Operator Lifecycle Manager (OLM)</li><li>Support for Aerospike Enterprise Server versions 5.6.x and 5.7.x</li><li>Support for Kubernetes 1.20, 1.21 and 1.22</li></ul><h3><strong>Breaking-changes</strong></h3><p>The Aerospike Kubernetes Operator 2.0 represents a major step forward in providing a complete control plane for Kubernetes-based Aerospike clusters. As we added new control capabilities, APIs inevitably had to change, and hence version 2.0 is not compatible with version 1.x. The cluster spec has breaking changes to accommodate increased flexibility and broader coverage of deployment options:</p><ul><li>More storage types like secrets, config maps, and empty dirs</li><li>Selective attachment of storage to containers (including init containers)</li><li>More control over pod scheduling using affinity/anti-affinity rules and tolerations</li><li>Ability to specify labels and annotations to Aerospike pods and attached persistent volumes</li></ul><h3><strong>RedHat OpenShift Container Platform Certification</strong></h3><p>The Aerospike Kubernetes Operator 2.0 is compatible with the RedHat OpenShift Container Platform and will be submitted for certification with this release and will be made available in the RedHat Ecosystem Catalog. The 2.0 version of our operator supports both Helm charts and OLM.</p><h3><strong>Documentation is available here:</strong></h3><p><a href="https://aerospike.github.io/kubernetes-operator">https://aerospike.github.io/kubernetes-operator</a></p><h3><strong>See our product page here:</strong></h3><p><a href="https://aerospike.com/products/kubernetes-operator/">https://aerospike.com/products/kubernetes-operator/</a></p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Introducing Aerospike Kubernetes Operator]]></title>
            <link>https://aerospike.github.io/kubernetes-operator/blog/introducing-aerospike-kubernetes-operator</link>
            <guid>introducing-aerospike-kubernetes-operator</guid>
            <pubDate>Mon, 30 Aug 2021 00:00:00 GMT</pubDate>
            <description><![CDATA[https://aerospike.com/blog/introducing-the-aerospike-kubernetes-operator/]]></description>
            <content:encoded><![CDATA[<p><a href="https://aerospike.com/blog/introducing-the-aerospike-kubernetes-operator/">https://aerospike.com/blog/introducing-the-aerospike-kubernetes-operator/</a></p>]]></content:encoded>
        </item>
    </channel>
</rss>